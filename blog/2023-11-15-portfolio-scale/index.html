<html>

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HQ9863ZP35"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HQ9863ZP35');
</script>
  <title>Capillaries blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link rel="shortcut icon" type="image/x-icon" href="../../favicon.ico">
  <link rel="stylesheet" href="../../css/bootstrap.min.css">
  <style>
    body {
      border-top: 4px solid #666;
    }

    h1 {
      font-weight: bold;
      float: left;
    }

    h2 {
      border-bottom: 1px solid #ccc;
      padding-bottom: .2em;
    }

    div.footer {
      border-top: 1px solid #ccc;
      padding: 1em 0 .5em;
      margin-top: 3em;
    }

    img {
      width: 100%;
      margin-top: 10px;
      margin-bottom: 10px;
    }

    img.logo {
      margin-left: 0px;
      margin-top: 0px;
      margin-right: 1em;
      margin-top: 20px;
    }
  </style>
</head>

<body>
  <div class="container">

    <div class="row" style="margin-top: 10px; margin-bottom: 20px;">
      <div class="col-lg-6">
        <img src="../../i/logo.svg" alt="Capillaries logo" class="pull-left logo" style="width:40px">
        <h1>Capillaries: notes</h1>
      </div>

      <div class="col-lg-3">
        <ul class="nav nav-pills pull-right" style="margin-top: 20px">
          <li><a href="../index.html">Blog home</a></li>
        </ul>
      </div>
      <div class="col-lg-3">
        <ul class="nav nav-pills pull-right" style="margin-top: 20px">
          <li><a href="https://github.com/capillariesio/capillaries">Source code and documentation</a></li>
        </ul>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h4>2023-11-15</h4>
        <h2>Capillaries: ARK portfolio performance calculation at scale</h2>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h3>The rationale</h3>
        <p>In the end of the day, using any piece of technology is about saving money. This document is an attempt to
          estimate the potential costs of implementing a Capillaries-based solution for <a
            href="https://capillaries.io/blog/2023-04-08-portfolio/index.html">portfolio performance calculation</a>. It
          mostly focuses on scalability aspects, while all calculation details have been covered already in <a
            href="https://capillaries.io/blog/2023-04-08-portfolio/index.html">that blog post</a>.</p>
        <p>This test give a realistic estimate of time and money an organization may need to allocate to produce
          portfolio performance results from raw data files, assuming that obtaining stock price for a specific date
          takes zero time and resources (involved stock quotes for 2020-2021 are hardcoded in the Python script that
          calculates portfolio performance).</p>
          <p>All AWS provisioning tasks were performed using Capillaries <a href="https://github.com/capillariesio/capillaries/tree/main/test/deploy">cloud deployment toolkit</a>. The code for preparing test data is in test/code/porfolio/bigtest, configuration data is in test/data/cfg/portfolio_bigtest.</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h3>Numbers and charts</h3>
        <p>The following tables and charts tell the story of 3 AWS-based test environments running the same
          <a href="https://github.com/capillariesio/capillaries/blob/main/test/data/cfg/portfolio_bigtest/script.json">portfolio_bigtest script</a> against the same raw data set:</p>
        <ul>
          <li>996 accounts stored in one 4kb Parquet file</li>
          <li>14,683,696 transactions stored in 500 4-6kb Parquet files</li>
          <li>713,800 end-of-month holding records stored in 10 4-6kb Parquet files</li>
        </ul>
        <p>The charts show Prometheus statistics for all 3 environments. Top graph is CPU usage, bottom
          graph is Cassandra writes/s. Open graphs in a separate browser tab to enlarge them.</p>
      </div>
    </div>



    <div class="row">
      <div class="col-lg-12">
        <h4>4 x c6a.8xlarge Cassandra cluster (128 cores)</h4>
        <hr />
      </div>
    </div>

    <div class="row">
      <div class="col-lg-7">
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <thead>
              <th class="text-right"></th>
              <th class="text-right">Flavor</th>
              <th class="text-right">Cores</th>
              <th class="text-right">RAM</th>
              <th class="text-right">Hourly USD</th>
              <th class="text-right">Qty</th>
              <th class="text-right">Hourly rate x qty</th>
              <th class="text-right">Total cores</th>
              <th class="text-right">Total RAM</th>
            </thead>
            <tbody>
              <tr>
                <td>Prometheus</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>RabbitMQ</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>Bastion</td>
                <td>c6a.large</td>
                <td>2</td>
                <td>4</td>
                <td>0.0765</td>
                <td>1</td>
                <td>$0.0765</td>
                <td>2</td>
                <td>4</td>
              </tr>
              <tr>
                <td>Cassandra</td>
                <td>c6a.8xlarge</td>
                <td>32</td>
                <td>64</td>
                <td>1.224</td>
                <td>4</td>
                <td>$4.8960</td>
                <td>128</td>
                <td>256</td>
              </tr>
              <tr>
                <td>Daemon<br />(24 workers, 16 writers)</td>
                <td>c6a.4xlarge</td>
                <td>16</td>
                <td>32</td>
                <td>0.612</td>
                <td>4</td>
                <td>$2.4480</td>
                <td>64</td>
                <td>128</td>
              </tr>
              <tr>
                <td>Total</td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td>11</td>
                <td>$7.4437</td>
                <td>196</td>
                <td>390</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <tbody>
              <tr>
                <td>Run times (sec)</td>
                <td>205,205,221</td>
              </tr>
              <tr>
                <td>Avg run time (sec)</td>
                <td><strong>210</strong></td>
              </tr>
              <tr>
                <td>Avg cost of run</td>
                <td><strong>$0.4349</strong></td>
              </tr>
              <tr>
                <td>Cost of setup (10 min)</td>
                <td>$1.2406</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
      <div class="col-lg-5">
        <img src="i/04.png" alt="">
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12">
        <h4>8 x c6a.8xlarge Cassandra cluster (256 cores)</h4>
        <hr />
      </div>
    </div>

    <div class="row">
      <div class="col-lg-7">
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <thead>
              <th class="text-right"></th>
              <th class="text-right">Flavor</th>
              <th class="text-right">Cores</th>
              <th class="text-right">RAM</th>
              <th class="text-right">Hourly USD</th>
              <th class="text-right">Qty</th>
              <th class="text-right">Hourly rate x qty</th>
              <th class="text-right">Total cores</th>
              <th class="text-right">Total RAM</th>
            </thead>
            <tbody>
              <tr>
                <td>Prometheus</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>RabbitMQ</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>Bastion</td>
                <td>c6a.large</td>
                <td>2</td>
                <td>4</td>
                <td>0.0765</td>
                <td>1</td>
                <td>$0.0765</td>
                <td>2</td>
                <td>4</td>
              </tr>
              <tr>
                <td>Cassandra</td>
                <td>c6a.8xlarge</td>
                <td>32</td>
                <td>64</td>
                <td>1.224</td>
                <td>8</td>
                <td>$9.7920</td>
                <td>256</td>
                <td>512</td>
              </tr>
              <tr>
                <td>Daemon (24 workers, 16 writers)</td>
                <td>c6a.4xlarge</td>
                <td>16</td>
                <td>32</td>
                <td>0.612</td>
                <td>8</td>
                <td>$4.8960</td>
                <td>128</td>
                <td>256</td>
              </tr>
              <tr>
                <td>Total</td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td>19</td>
                <td>$14.7877</td>
                <td>388</td>
                <td>774</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <tbody>
              <tr>
                <td>Run times (sec)</td>
                <td>135,126,126</td>
              </tr>
              <tr>
                <td>Avg run time (sec)</td>
                <td><strong>129</strong></td>
              </tr>
              <tr>
                <td>Avg cost of run</td>
                <td><strong>$0.5299</strong></td>
              </tr>
              <tr>
                <td>Cost of setup (10 min)</td>
                <td>$2.4646</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
      <div class="col-lg-5">
        <img src="i/08.png" alt="">
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12">
        <h4>16 x c6a.8xlarge Cassandra cluster (512 cores)</h4>
        <hr />
      </div>
    </div>

    <div class="row">
      <div class="col-lg-7">
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <thead>
              <th class="text-right"></th>
              <th class="text-right">Flavor</th>
              <th class="text-right">Cores</th>
              <th class="text-right">RAM</th>
              <th class="text-right">Hourly USD</th>
              <th class="text-right">Qty</th>
              <th class="text-right">Hourly rate x qty</th>
              <th class="text-right">Total cores</th>
              <th class="text-right">Total RAM</th>
            </thead>
            <tbody>
              <tr>
                <td>Prometheus</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>RabbitMQ</td>
                <td>t2.micro</td>
                <td>1</td>
                <td>1</td>
                <td>0.0116</td>
                <td>1</td>
                <td>$0.0116</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>Bastion</td>
                <td>c6a.large</td>
                <td>2</td>
                <td>4</td>
                <td>0.0765</td>
                <td>1</td>
                <td>$0.0765</td>
                <td>2</td>
                <td>4</td>
              </tr>
              <tr>
                <td>Cassandra</td>
                <td>c6a.8xlarge</td>
                <td>32</td>
                <td>64</td>
                <td>1.224</td>
                <td>16</td>
                <td>$19.5840</td>
                <td>512</td>
                <td>1024</td>
              </tr>
              <tr>
                <td>Daemon<br />(24 workers, 16 writers)</td>
                <td>c6a.4xlarge</td>
                <td>16</td>
                <td>32</td>
                <td>0.612</td>
                <td>16</td>
                <td>$9.7920</td>
                <td>256</td>
                <td>512</td>
              </tr>
              <tr>
                <td>Total</td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td><br></td>
                <td>35</td>
                <td>$29.4757</td>
                <td>772</td>
                <td>1542</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="row no-gutters">
          <table class="table table-striped text-right" style="font-size:smaller">
            <tbody>
              <tr>
                <td>Run times (sec)</td>
                <td>89,99,93,91,93</td>
              </tr>
              <tr>
                <td>Avg run time (sec)</td>
                <td><strong>93</strong></td>
              </tr>
              <tr>
                <td>Avg cost of run</td>
                <td><strong>$0.7615</strong></td>
              </tr>
              <tr>
                <td>Cost of setup (10 min)</td>
                <td>$4.9126</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
      <div class="col-lg-5">
        <img src="i/16.png" alt="">
      </div>
    </div>






    <div class="row">
      <div class="col-lg-12">
        <p>There are a few distinctive areas on each scenario graph</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h4>ETL: load transactions</h4>
        <p>This is the plateau in the beginning. Daemons with 64, 128, or 256 total cores load 500 parquet files with 14
          million transactions. Cassandra nodes are at ~90%, daemon instances at 50-60%. Also, account and holding
          data
          is loaded, but the amounts are a few orders of magnitude smaller, they can be ignored.</p>
        <p>Cassandra writes per second are at 300K, 650K, 1.35m</p>
        <h4>Joins: for each account, all correspondent transactions are combined into a single JSON string</h4>
        <p>This is where the CPU usage drops, details discussed below. Writes are very few.
        </p>
        <h4>Python calculations</h4>
        <p>Daemon instances are at 100% CPU running Python code for portfolio performance calculation for 996 JSON strings. Database activity is almost zero.</p>
        <h4>Producing summaries</h4>
        <p>Daemon instances produce totals, CPU usage 40-90%. Database activity at minimum.</p>
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12">
        <h3>Cassandra tokens and CPU usage patterns</h3>
        <p></p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h4>Why can't we have 100% CPU usage when we do joins?</h4>
        <p>All 3 runs of the 8-node scenario were web-scraped and saved <a href="./scrape-8/-hash-.html">here</a>. Let's
          take a closer look at the step <a
            href="scrape-8/-hash-ks-portfolio_bigtest-run-3-node-2_account_txns_outer-batch_history.html"><strong>2_account_txns_outer</strong>
            in run 3</a>. Its execution timeline is below. Open it in a separate
          browser tab, zoom in to see individual lines and move mouse pointer over them to see batch number and
          execution time - the longer the execution takes, the longer the line:</p>
        <img src="i/1000-batch-txn-join.svg" alt="" />
        <p> Some lines are merely points - execution took only a fraction of a second. Let's get the
          numbers by looking for "read 0:", "read: 1", "read: 2", "read: 3", "read: 4", "read: 5", and "read: 6" on the <a
          href="scrape-8/-hash-ks-portfolio_bigtest-run-3-node-2_account_txns_outer-batch_history.html">batch history page</a>:</p>
        <ul>
          <li>batches with 0 items: 363 occurrences</li>
          <li>batches with 1 item: 367 occurrences</li>
          <li>batches with 2 items: 203 occurrences</li>
          <li>batches with 3 items: 51 occurrences</li>
          <li>batches with 4 items: 10 occurrences</li>
          <li>batches with 5 items: 6 occurrences</li>
          <li>batches with 6 items: 0 occurrences</li>
        </ul>
        <p>
          A batch with zero items means: daemon worker looked for accounts with primary key hash (Cassandra token)
          between lower and upper
          boundary for this batch, and found nothing. This batch turned out empty and no records were generated as a
          result. A batch with one item means there was one account with primary key hash matching batch criteria,
          daemon worker looked for all transactions for this account, found a few thousand and created an out put record
          for this account. Two items in the batch means two accounts, and so on.
        </p>
        <p>There are two problems here.</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-8">
        <h5>1. A few "heavy" batches</h5>
        <p>Each of those 51*3 + 10*4 + 6*5 = 223 "heavy" batches with 3,4 and 5 items (accounts) is processed as a
          single atomic task by Capillaries daemon workers. So, in our stress test environment, daemon threads that end
          up processing batches with 0,1, or 2 items finish their job early and have nothing to do, while those
          processing batches with 3,4, and 5 items keep working for a much longer time. In a real production
          environment, those "light-weighted" 1,2,3-item threads would be busy with other tasks created by other
          Capillaries scripts (maybe unrelated to porfolio
          performance calculation). But in our scenario, they just do nothing, driving our overall CPU usage down.
        </p>
      </div>
      <div class="col-lg-4">
        <h5>2. A lot of empty batches</h5>
        <p>About a third of the batches ends up with no accounts in them. When processing a batch like this, daemon
          worker thread queries the database for the account data, waits for the result, receives empty dataset and
          declares the batch complete. No actual join is performed.</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <p>Let's generalize our findings. The test works with <strong>996</strong> accounts, and the
          number of batches specified for <strong>2_account_txns_outer</strong> by the script is
          <strong>1000</strong>:
        </p>
        <pre>
  "<strong>2_account_txns_outer</strong>": {
    "type": "table_lookup_table",
    "desc": "...",
    "r": {
        "table": "accounts",
        <strong>"expected_batches_total": 1000</strong>
    },
    ...
  }
          </pre>
        <p>
          Let's calculate the theoretical probability of a batch (it's also convenient to call it a bucket as they
          do in computer science) receiving a
          specified amount of items uniformely distributed across the domain. We will do that for 100,
          <strong>1000</strong> (our case), and 10000 buckets. We can do that with a <a href="poisson.html">simple
            Python program.</a>
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <p>The output is below. As you can see, our findings for the 1000-bucket item distribution match the
          theoretical numbers produced by this Python program pretty closely - those "heavy" buckets/batches are
          <mark>highlighted</mark>:
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-4" style="padding-left:1px;padding-right:1px;">
        <pre style="font-family:monospace;color: green;">
100 buckets, 996 items (Poisson λ=9.96)

0-item buckets: 0.004, P=0.0000449, 0.00449%
1-item buckets: 0.045, P=0.0004521, 0.04521%
2-item buckets: 0.227, P=0.0022721, 0.22721%
3-item buckets: 0.760, P=0.0076044, 0.76044%
4-item buckets: 1.907, P=0.0190686, 1.90686%
5-item buckets: 3.821, P=0.0382143, 3.82143%
6-item buckets: 6.375, P=0.0637549, 6.37549%
7-item buckets: 9.108, P=0.0910784, 9.10784%
8-item buckets: 11.373, P=0.1137330, 11.37330%
9-item buckets: 12.611, P=0.1261147, 12.61147%
10-item buckets: 12.573, P=0.1257326, 12.57326%
11-item buckets: 11.384, P=0.1138405, 11.38405%
12-item buckets: 9.439, P=0.0943880, 9.43880%
13-item buckets: 7.217, P=0.0721661, 7.21661%
14-item buckets: 5.118, P=0.0511827, 5.11827%
15-item buckets: 3.385, P=0.0338461, 3.38461%
16-item buckets: 2.096, P=0.0209615, 2.09615%
17-item buckets: 1.221, P=0.0122057, 1.22057%
18-item buckets: 0.671, P=0.0067056, 0.67056%
19-item buckets: 0.349, P=0.0034865, 0.34865%
            </pre>
      </div>
      <div class="col-lg-4" style="padding-left:1px;padding-right:1px;">
        <pre style="font-family:monospace;color: red">
1000 buckets, 996 items (Poisson λ=0.996)

0-item buckets: 369.170, P=0.3691699, 36.91699%
1-item buckets: 368.061, P=0.3680613, 36.80613%
2-item buckets: 183.294, P=0.1832938, 18.32938%
<mark>3-item buckets: 60.792, P=0.0607921, 6.07921%
4-item buckets: 15.107, P=0.0151068, 1.51068%
5-item buckets: 3.000, P=0.0030002, 0.30002%
6-item buckets: 0.496, P=0.0004960, 0.04960%</mark>
7-item buckets: 0.070, P=0.0000702, 0.00702%
8-item buckets: 0.009, P=0.0000087, 0.00087%
9-item buckets: 0.001, P=0.0000010, 0.00010%
10-item buckets: 0.000, P=0.0000001, 0.00001%
11-item buckets: 0.000, P=0.0000000, 0.00000%
12-item buckets: 0.000, P=0.0000000, 0.00000%
13-item buckets: 0.000, P=0.0000000, 0.00000%
14-item buckets: 0.000, P=0.0000000, 0.00000%
15-item buckets: 0.000, P=0.0000000, 0.00000%
16-item buckets: 0.000, P=0.0000000, 0.00000%
17-item buckets: 0.000, P=0.0000000, 0.00000%
18-item buckets: 0.000, P=0.0000000, 0.00000%
19-item buckets: 0.000, P=0.0000000, 0.00000%
            </pre>
      </div>
      <div class="col-lg-4" style="padding-left:1px;padding-right:1px;">
        <pre style="font-family:monospace;color: blue">
10000 buckets, 996 items (Poisson λ=0.0996)

0-item buckets: 9051.949, P=0.9051949, 90.51949%
1-item buckets: 901.664, P=0.0901664, 9.01664%
2-item buckets: 44.862, P=0.0044862, 0.44862%
3-item buckets: 1.487, P=0.0001487, 0.01487%
4-item buckets: 0.037, P=0.0000037, 0.00037%
5-item buckets: 0.001, P=0.0000001, 0.00001%
6-item buckets: 0.000, P=0.0000000, 0.00000%
7-item buckets: 0.000, P=0.0000000, 0.00000%
8-item buckets: 0.000, P=0.0000000, 0.00000%
9-item buckets: 0.000, P=0.0000000, 0.00000%
10-item buckets: 0.000, P=0.0000000, 0.00000%
11-item buckets: 0.000, P=0.0000000, 0.00000%
12-item buckets: 0.000, P=0.0000000, 0.00000%
13-item buckets: 0.000, P=0.0000000, 0.00000%
14-item buckets: 0.000, P=0.0000000, 0.00000%
15-item buckets: 0.000, P=0.0000000, 0.00000%
16-item buckets: 0.000, P=0.0000000, 0.00000%
17-item buckets: 0.000, P=0.0000000, 0.00000%
18-item buckets: 0.000, P=0.0000000, 0.00000%
19-item buckets: 0.000, P=0.0000000, 0.00000%
            </pre>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <p>If we want to minimize the probability of getting batches with 3 and more items, we
          have to substantially increase the number of batches. And we have to pay for that with CPU, network, queue
          and database resources spent on empty batches (for example, ~90% of the batches are empty if 996-items are
          spread
          among 10000-batches). And remember: it's
          just a probability, so there is no guarantee that those "heavy" batches do not occur once in a while.</p>
        <p>For example, CPU usage pattern doesn't change much even when the number of batches is 10000, and we get
          these frequencies:</p>
        <ul>
          <li>batches with 0 items: 9052 occurrences</li>
          <li>batches with 1 item: 901 occurrences</li>
          <li>batches with 2 items: 46 occurrences</li>
          <li>batches with 3 items: 3 occurrences</li>
        </ul>
        <p>The <a href="i/10000-batch-txn-join.svg" alt="">2_account_txns_outer 10000-batch
            execution
            timeline</a> answers the question why CPU usage on this step is still around 40% (open this link in a
          separate tab and zoom in to the maximum): for 90% of the batches, daemon worker thread just queries
          Cassandra
          for the account info, waits for the result, finds nothing and declares the batch processed. Perhaps, if
          the
          network+database
          latency
          was lower, daemon worker threads would be busier.</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h4>Bonus chapter: Poisson distribution</h4>
        <p>The challenge we are facing here is very well-known among hash table implementers: when a bucket in a
          hash table receives too many items (the word <i>collisions</i>is used) the implementation should resolve
          them without allowing hash table performance to degrade to O(n). Hash table developers know: when the
          number of items and buckets is large, the frequency of items in buckets follows a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>. This
          helps them
          choose proper parameters for their implementations.</p>
        <p>We can forget about the Python program above, and estimate the number of items in a bucket
          using this graph for <span style="color:00A000;">100 (green)</span>, <span style="color:E00000;">1000
            (red)</span> and <span style="color:0000FF;">10000 (blue)</span> buckets (same color scheme as Python
          program
          output above). This graph engine doesn't allow greeks, so <i>a</i>, <i>b</i> and <i>c</i> were used instead of Poisson distribution
          &lambda;:</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <img src="i/poisson.png" alt="" />
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <p>From the graph: larger number of buckets (smaller <strong>&lambda;</strong> in Poisson distribution) leads to
          more empty buckets and fewer buckets with item count > 1. Otherwise, setups with a smaller number of buckets get us very
          few empty buckets, but a lot of "heavy" buckets. Both empty and "heavy" buckets are bad if we want to get 100% CPU usage. Pick your <i>poison</i>.</p>
        <p>One of the most critical statistics for hash tables is "load factor": the number of entries occupied in
          the hash table divided by is the number of buckets. In our case, it's 996/1000=0.996. And this is the
          <strong>&lambda;</strong> in Poisson distribution formula. Acceptable figures of hash table load factor
          range around 0.6 to 0.75 (a lot of empty buckets and a suppressed "tail" of the Poisson distribution curve).
          To keep
          load factor within this range, hash table implementation may use
          re-hashing techniques that change the number of buckets.
        </p>
        <p>Can we do something like that in Capillaries? For example, we could create buckets (batches) of different
          sizes to spread items evenly across buckets. But that would require an extra pre-run against all items in
          Cassandra table, and it probably has be an atomic operation, therefore not scalable. Another potential soulution
          would be using some low-latency techniques
          (like Redis counters) for marking empty batches: 1_read_accounts step marks empty batches, and
          2_account_txns_outer declares them complete without the costly Cassandra roundrip. A potential Capillaries improvement to think about.</p>
      </div>
    </div>

    <div class=" footer">
      <p>&copy;
        <script type="text/javascript">document.write("2022-" + new Date().getFullYear());</script>
        Capillaries.io
      </p>
    </div>

  </div>
</body>

</html>